{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4288f71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import random\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler,normalize\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.optimizers\n",
    "from tensorflow.keras.layers import Flatten,concatenate,Dense,MaxPooling2D,Conv2D,BatchNormalization\n",
    "import radiomics\n",
    "import SimpleITK as sitk\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e496d37a",
   "metadata": {},
   "source": [
    "# Load the Training and Testing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d86cbe",
   "metadata": {},
   "source": [
    "We load CT scans from two folders, also I sorted the return list just keep the load in the same order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5d27e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label index 0 is positive, 1 is negative\n",
    "CLASS_NAMES  = ['pos', 'neg']\n",
    "DATA_DIRECTORY  = './covid19/kcv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7695eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_training():\n",
    "    X, Y = [], []\n",
    "    for label, class_name in enumerate(CLASS_NAMES):\n",
    "        class_dir = os.path.join(DATA_DIRECTORY, class_name)\n",
    "        for filename in sorted(os.listdir(class_dir)):\n",
    "            img_path = os.path.join(class_dir, filename)\n",
    "            img = sitk.ReadImage(img_path)\n",
    "            img_array = sitk.GetArrayFromImage(img).reshape(512, 512, 1)\n",
    "            X.append(img_array)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b54d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y= load_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b86d41e",
   "metadata": {},
   "source": [
    "We load the features we previous extracted from the images (processure from the above session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b7010c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(feature_files):\n",
    "    return [np.load(file) for file in feature_files]\n",
    "\n",
    "feature_files = [\n",
    "    'original_firstorder_Skewness.npy', 'original_glcm_Autocorrelation.npy',\n",
    "    'original_glrlm_GrayLevelVariance.npy', 'original_glszm_SizeZoneNonUniformity.npy',\n",
    "    'original_glcm_ClusterShade.npy', 'original_glcm_DifferenceEntropy.npy'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e51e94",
   "metadata": {},
   "source": [
    "Apply to k fold groups, in this case I am using n =10,\n",
    "then i am going to prepare the input and the output data for the 10 groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fc39e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_kfold_data(X, Y, feature_arrays, n_splits=10):\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "    data = {f'train_{i}': [] for i in range(1, 7)}\n",
    "    data.update({f'test_{i}': [] for i in range(1, 7)})\n",
    "    data['train_group'], data['test_group'] = [], []\n",
    "    data['train_labels'], data['test_labels'] = [], []\n",
    "\n",
    "    for train_idx, test_idx in kfold.split(X):\n",
    "        for i, feature_array in enumerate(feature_arrays, 1):\n",
    "            data[f'train_{i}'].append(feature_array[train_idx])\n",
    "            data[f'test_{i}'].append(feature_array[test_idx])\n",
    "        data['train_group'].append([X[idx] for idx in train_idx])\n",
    "        data['test_group'].append([X[idx] for idx in test_idx])\n",
    "        data['train_labels'].append([Y[idx] for idx in train_idx])\n",
    "        data['test_labels'].append([Y[idx] for idx in test_idx])\n",
    "\n",
    "    return {k: np.array(v) for k, v in data.items()}\n",
    "\n",
    "features = load_features(feature_files)\n",
    "datasets = prepare_kfold_data(X, Y, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b8c32",
   "metadata": {},
   "source": [
    "# FINs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d08d21c",
   "metadata": {},
   "source": [
    "A essemble of FINs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c06f9c",
   "metadata": {},
   "source": [
    "The part we load the FINs attacted to a DFNN and compare with the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39d0c56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_rename_models(model_paths):\n",
    "    models = {}\n",
    "    for name, path in model_paths.items():\n",
    "        model = keras.models.load_model(path)\n",
    "        model._name = name\n",
    "        models[name] = model\n",
    "    return models\n",
    "\n",
    "def make_FIN(models, input_shape=(512, 512, 1), model_name=\"FINsEssemble\"):\n",
    "    input_layer = keras.Input(shape=input_shape)\n",
    "    \n",
    "    # Utilize the preloaded models\n",
    "    outputs = [model(input_layer) for model in models.values()]\n",
    "    \n",
    "    # CNN architecture\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')(input_layer)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # Concatenate all outputs\n",
    "    concatenated = concatenate([*outputs, x])\n",
    "    x = BatchNormalization()(concatenated)\n",
    "    x = Dense(32, activation='relu', kernel_initializer=tf.keras.initializers.GlorotNormal())(x)\n",
    "    x = Dense(32, activation='relu', kernel_initializer=tf.keras.initializers.GlorotNormal())(x)\n",
    "    x = Dense(32, activation='relu', kernel_initializer=tf.keras.initializers.GlorotNormal())(x)\n",
    "    \n",
    "    final_output = Dense(1, activation='sigmoid', name=model_name)(x)\n",
    "    model = keras.Model(inputs=input_layer, outputs=final_output, name=model_name)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Define model paths\n",
    "model_paths = {\n",
    "    'feature1': 'Skewness5.h5',\n",
    "    'feature2': 'Autocorrelation5.h5',\n",
    "    'feature3': 'GrayLevelVariance5.h5',\n",
    "    'feature4': 'SizeZoneNonUniformity5.h5',\n",
    "    'feature5': 'ClusterShade5.h5',\n",
    "    'feature6': 'DifferenceEntropy5.h5'\n",
    "}\n",
    "\n",
    "# Load and rename models\n",
    "models = load_and_rename_models(model_paths)\n",
    "\n",
    "# Build the composite model\n",
    "FinModel = make_FIN(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a728b5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryLogger(Callback):\n",
    "    \"\"\" Custom callback to log the history of training. \"\"\"\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "        print(f\"Accuracy: {logs.get('accuracy')}, Loss: {logs.get('loss')}\")\n",
    "        if 'val_accuracy' in logs:\n",
    "            print(f\"Validation Accuracy: {logs.get('val_accuracy')}, Validation Loss: {logs.get('val_loss')}\")\n",
    "            \n",
    "history_logger = HistoryLogger()\n",
    "\n",
    "train_images = datasets['train_group'][7]  # Example: using the 8th fold\n",
    "train_labels = datasets['train_labels'][7]\n",
    "test_images = datasets['test_group'][7]\n",
    "test_labels = datasets['test_labels'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0dfdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "FinModel.compile(optimizer=tf.keras.optimizers.Adam(\n",
    "     learning_rate= 0.001\n",
    "),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "             metrics=['accuracy',tf.keras.metrics.AUC()]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f332793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_fin = FinModel.fit(\n",
    "    train_images, train_labels,\n",
    "    validation_data=(test_images, test_labels),\n",
    "    batch_size=64,\n",
    "    epochs=10,\n",
    "    callbacks=[history_logger],\n",
    "    shuffle=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
